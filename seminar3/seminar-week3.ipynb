{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Natural Language Processing</center></h1>\n",
    "<h2><center>Week3 Seminar - embeddings</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Самые известные модели\n",
    "## GloVe - Global VEctors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Шаг 1. Собрать матрицу совстречаемости слов в корпусе (или матрицу PMI)\n",
    "$$ PMI_{uv} = \\frac{n_{vu}n}{n_v n_u} $$\n",
    "* Шаг 2. Разложить эту матрицу на произведение двух других матриц\n",
    "\n",
    "<center><img src='https://miro.medium.com/max/19360/1*yNFtSAwoht-nlfzW_Um_yw.png' width=600></center>\n",
    "\n",
    "Как это можно сделать?\n",
    "\n",
    "* Использовать типовые алгоритмы матричной факторизации (NMF, SVD..)\n",
    "* Использовать градиентный спуск для какой-нибудь функции потерь\n",
    "$$L = \\sum\\limits_{u,v} f(n_{uv})   (\\phi_u^T \\theta_v + b_u + \\tilde{b}_v - \\text{log}   n_{uv})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Слово предсказывает слова из своего контекста\"\n",
    "* Для каждого слова инициализируем вектор-ембеддинг\n",
    "* Обновляем их таким образом, чтобы вектор текущего слова был похож на вектора окружения\n",
    "$$ L = \\frac{1}{T}\\sum\\limits_{t=1}^T\\ \\sum\\limits_{-n \\leq j \\leq n , \\neq 0} \\text{log} \\space p(w_{t+j} \\: | \\: w_t) $$\n",
    "$$ p(w_{t+j} \\: | \\: w_t ) = \\dfrac{\\text{exp}({v_{w_t}^\\top v'_{w_{t+j}}})}{\\sum_{w_i \\in V} \\text{exp}({v_{w_t}^\\top v'_{w_i}})} $$\n",
    "\n",
    "<center><img src='http://ruder.io/content/images/2016/02/skip-gram.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW - Continious Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Комбинация слов контекста предсказывает текущее слово\"\n",
    "* Для каждого слова инициализируем вектор-ембеддинг\n",
    "* Обновляем их таким образом, чтобы комбинация векторов контекста была наиболее похожа на текущее слово"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L = \\frac{1}{T}\\sum\\limits_{t=1}^T\\ \\text{log} \\space p(w_t \\: | \\: w_{t-n} , \\cdots , w_{t-1}, w_{t+1}, \\cdots , w_{t+n})$$\n",
    "$$ p(w_t \\: |\\: w_{t-n} , \\cdots , w_{t-1}, w_{t+1}, \\cdots , w_{t+n} ) = \\dfrac{\\text{exp}({v_{w_t}^\\top u_t})}{\\sum_{w_i \\in V} \\text{exp}({v_{w_t}^\\top u_t})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='http://ruder.io/content/images/2016/02/cbow.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительная информация - [туть](https://cs224d.stanford.edu/lecture_notes/notes1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные практические проблемы\n",
    "* Out of vocabulary words\n",
    "* Векторные представления н-грамм, предложений или текстов\n",
    "* Применимость word2vec as is в моделях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!wget https://www.dropbox.com/s/jrhs1gxcql57r73/amazon_cells_labelled.txt?dl=0 -O ./data/amazon_cells_labelled.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.read_csv('./data/amazon_cells_labelled.txt', sep='\\t', names=['review', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стандартная нормализация текстов без изысков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приводим к нижнему регистру\n",
    "df_text.loc[:, 'review'] = df_text.review.str.lower()\n",
    "# Токенизируем\n",
    "df_text.loc[:, 'review'] = df_text.review.apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_texts = df_text.review.values\n",
    "y = df_text.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если бы вы хотели обучать свой word2vec\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "# model = Word2Vec(your_corpus, \n",
    "#                  size=32,      \n",
    "#                  min_count=5,  \n",
    "#                  window=5).wv \n",
    "\n",
    "# Но мы используем уже предобученные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список предобученных эмбедингов можно найти [тут](https://github.com/RaRe-Technologies/gensim-data#models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['microphone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('microphone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['coder', 'money'], negative=['brain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.vocab.keys())[:30000]\n",
    "\n",
    "print(words[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.array([model[w] for w in words])\n",
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, n_jobs=4)\n",
    "tsne.fit(word_vectors)\n",
    "\n",
    "word_vectors_tsne = tsne.embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    if isinstance(color, str): color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(word_vectors_tsne[:, 0], word_vectors_tsne[:, 1], token=words)\n",
    "\n",
    "# hover a mouse over there and see if you can identify the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Напишите функцию, которая на вход будет брать список слов, а на выходе будет возвращать усредненный вектор из эмбеддингов\n",
    "# Если слова не было в словаре, то при расчете среднего оно не должно учитываться!\n",
    "\n",
    "def mean_word_vectors(text, model_wv, emb_size):\n",
    "    mean_vector = np.zeros(emb_size)\n",
    "    \n",
    "    ## Your code here\n",
    "    \n",
    "    return mean_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embeddings = np.array([mean_word_vectors(t, model, model.vector_size) for t in X_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_embeddings ,y, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict_proba(X_valid)\n",
    "\n",
    "roc_auc_score(y_valid, y_hat[:, 1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "142px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
